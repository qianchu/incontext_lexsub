{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def context_independent_sim(x,y):\n",
    "# # use batch_to_ids to convert sentences to character ids\n",
    "#     sentences = [[x], [y]]\n",
    "#     character_ids = batch_to_ids(sentences)\n",
    "#     embeddings = elmo(character_ids)\n",
    "#     x1=embeddings['elmo_representations'][0][0].detach().numpy()\n",
    "#     x2=embeddings['elmo_representations'][0][1].detach().numpy()\n",
    "# #     print (x1)\n",
    "#     return cosine_similarity(x1,x2)\n",
    "    \n",
    "# allennlp.modules.scalar_mix.ScalarMix??\n",
    "\n",
    "\n",
    "# def context_independent(x):\n",
    "#     sentences=[[x]]\n",
    "#     character_ids = batch_to_ids(sentences)\n",
    "#     embeddings = elmo(character_ids)\n",
    "#     x1=embeddings['elmo_representations'][0][0].detach().numpy()\n",
    "#     return x1\n",
    "\n",
    "\n",
    "def context_independent_batch(words,batchsize):\n",
    "    words_matrix=[]\n",
    "    for i in range(int(len(words)/batchsize)+1):\n",
    "        print (i)\n",
    "        start=i*batchsize\n",
    "        end=(i+1)*batchsize\n",
    "        if end > len(words):\n",
    "            end=len(words)\n",
    "        sentences=[[w] for w in words[start:end]]\n",
    "        character_ids = batch_to_ids(sentences)\n",
    "        embeddings = elmo(character_ids)\n",
    "        x_ls=embeddings['elmo_representations'][0].detach().numpy()\n",
    "        words_matrix.append(x_ls)\n",
    "        \n",
    "    words_matrix_all=np.concatenate(words_matrix,axis=0)\n",
    "#     words_matrix=words_matrix.reshape(words_matrix.shape[0],words_matrix.shape[2])\n",
    "    return words_matrix_all\n",
    "    \n",
    "    \n",
    "def context_dependent(x,index=0):\n",
    "    sentences=[x]\n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    embeddings = elmo(character_ids)\n",
    "    print (embeddings['elmo_representations'][0].shape)\n",
    "    x1=embeddings['elmo_representations'][0][0][index].detach().numpy()\n",
    "    return x1\n",
    "\n",
    "def nearest_neighbour(x,vocab,index2word,n_result,index=0):\n",
    "    vocab = normalize(vocab)\n",
    "    x1=context_dependent(x,index)\n",
    "    x1=normalize([x1])[0]\n",
    "#     x1 = x1 / np.sqrt((x1 * x1).sum())\n",
    "\n",
    "#     x1=x1[0]\n",
    "#     print(np.linalg.norm(x1))\n",
    "#     similarity=cosine_similarity2([x1],vocab)[0]\n",
    "    similarity=vocab.dot(x1)\n",
    "    count=0\n",
    "    for i in (-similarity).argsort():\n",
    "                    if np.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    print('{0}: {1}'.format(str(index2word[i]), str(similarity[i])))\n",
    "                    count += 1\n",
    "#                     top_words_i.append(i)\n",
    "#                     top_words.append(index2word[i])\n",
    "#                     similarity_scores.append(similarity[i])\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0a196082f6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         vocab_matrix.append(context_independent(word))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#vocab\n",
    "\n",
    "word2index={}\n",
    "index2word={}\n",
    "vocab_matrix=[]\n",
    "words=[]\n",
    "index=0\n",
    "with open('../corpora/1-billion-vocab') as f:\n",
    "    for line in f:\n",
    "        word=line.split()[0]\n",
    "#         vocab_matrix.append(context_independent(word))\n",
    "        index2word[int(index)]=word\n",
    "        word2index[word]=int(index)\n",
    "        words.append(word)\n",
    "        index+=1\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "words_matrix=context_independent_batch(words[:10000],1000)\n",
    "len(words_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2elmo=np.load('./lexsub_en/vocab2elmo.npy')\n",
    "vocab2index=np.load('./lexsub_en/vocab2index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200065"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2vocab={}\n",
    "vocab2index_new={}\n",
    "for vocab_index in vocab2index:\n",
    "    vocab2index_new[vocab_index[0]]=int(vocab_index[1])\n",
    "    index2vocab[int(vocab_index[1])]=vocab_index[0]\n",
    "index2vocab\n",
    "len(vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity simlex\n",
    "predicts=[]\n",
    "golds=[]\n",
    "line_num=0\n",
    "with open ('simlex') as f:\n",
    "    for line in f:\n",
    "        if line_num==0:\n",
    "            line_num+=1\n",
    "            continue\n",
    "        \n",
    "        line=line.split('\\t')\n",
    "        try:\n",
    "            predict = cosine_similarity(words_matrix[word2index[line[0]]],words_matrix[word2index[line[1]]])\n",
    "        except KeyError as e:\n",
    "            predict = cosine_similarity([context_dependent([line[0]])],[context_dependent([line[1]])])\n",
    "        \n",
    "        print (line[3],predict[0][0])\n",
    "\n",
    "        predicts.append(predict[0][0])\n",
    "\n",
    "        golds.append(line[3])\n",
    "\n",
    "        line_num+=1   \n",
    "          \n",
    "print ('simlex sim is {0}'.format(spearmanr(predicts,golds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEN\n",
    "predicts=[]\n",
    "golds=[]\n",
    "line_num=0\n",
    "with open ('MEN_dataset_lemma_form_full','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "    #         if line_num==0:\n",
    "#             line_num+=1\n",
    "#             continue\n",
    "        line=line.split()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            predict = cosine_similarity(words_matrix[word2index[line[0].split('-')[0]]],words_matrix[word2index[line[1].split('-')[0]]])\n",
    "        except KeyError as e:\n",
    "#             print (line[0].split('-')[0])\n",
    "            predict = cosine_similarity([context_dependent([line[0].split('-')[0]])],[context_dependent([line[1].split('-')[0]])])\n",
    "        \n",
    "        print (line[2],predict[0][0])\n",
    "\n",
    "        predicts.append(predict[0][0])\n",
    "\n",
    "        golds.append(float(line[2]))\n",
    "\n",
    "        line_num+=1   \n",
    "          \n",
    "print ('MEN sim is {0}'.format(spearmanr(predicts,golds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "def cosine_similarity2(X, Y=None, dense_output=True):\n",
    "    \"\"\"Compute cosine similarity between samples in X and Y.\n",
    "\n",
    "    Cosine similarity, or the cosine kernel, computes similarity as the\n",
    "    normalized dot product of X and Y:\n",
    "\n",
    "        K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "\n",
    "    On L2-normalized data, this function is equivalent to linear_kernel.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cosine_similarity>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray or sparse array, shape: (n_samples_X, n_features)\n",
    "        Input data.\n",
    "\n",
    "    Y : ndarray or sparse array, shape: (n_samples_Y, n_features)\n",
    "        Input data. If ``None``, the output will be the pairwise\n",
    "        similarities between all samples in ``X``.\n",
    "\n",
    "    dense_output : boolean (optional), default True\n",
    "        Whether to return dense output even when the input is sparse. If\n",
    "        ``False``, the output is sparse if both input arrays are sparse.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           parameter ``dense_output`` for dense output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kernel matrix : array\n",
    "        An array with shape (n_samples_X, n_samples_Y).\n",
    "    \"\"\"\n",
    "    # to avoid recursive import\n",
    "\n",
    "#     X, Y = check_pairwise_arrays(X, Y)\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    X_normalized = X / np.sqrt((X * X).sum())\n",
    "    if X is Y:\n",
    "        Y_normalized = X_normalized\n",
    "    else:\n",
    "        Y_normalized = Y / np.sqrt((Y * Y).sum())\n",
    "    \n",
    "    print (np.linalg.norm(X_normalized),np.linalg.norm(Y_normalized[0]))\n",
    "    K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['during', 'the', 'siege', ',', 'george', 'robertson', 'had', 'appointed', 'shuja-ul-mulk', ',', 'who', 'was', 'a', 'bright', 'boy', 'only', '12', 'years', 'old', 'and', 'the', 'youngest', 'surviving', 'son', 'of', 'aman-ul-mulk', ',', 'as', 'the', 'ruler', 'of', 'chitral', '.'] 13\n",
      "torch.Size([1, 33, 1024])\n",
      "bright: 0.6178409717950697\n",
      "brightest: 0.44194920037417634\n",
      "brighter: 0.4345869818496767\n",
      "brightly: 0.434375261264788\n",
      "willowy: 0.43242943203422946\n",
      "brights: 0.4223119771353663\n",
      "cute: 0.4111005022496549\n",
      "lithe: 0.4102189820439516\n",
      "blingy: 0.4074446646741163\n",
      "dandyish: 0.40685478218358534\n",
      "petite: 0.4059724243662819\n",
      "brainy: 0.4051443339616425\n",
      "dreamy: 0.4049909645208598\n",
      "blonde: 0.4049773692461775\n",
      "cheerful: 0.40490147522703557\n",
      "crocky: 0.40411901112854276\n",
      "bruery: 0.4031415073969038\n",
      "ditsy: 0.40281840452228734\n",
      "natty: 0.40123202370614097\n",
      "raffish: 0.4009306764528258\n",
      "jowly: 0.4006712403478009\n",
      "waffly: 0.3999965760430098\n",
      "brash: 0.39966043897587367\n",
      "shiny: 0.3984426917319783\n",
      "blondy: 0.3981500847146695\n",
      "blond: 0.39763486670774106\n",
      "dewy: 0.39756840025336937\n",
      "lanky: 0.3962064689475533\n",
      "croaky: 0.394909710578344\n",
      "bleek: 0.39472719061261374\n"
     ]
    }
   ],
   "source": [
    "# nearest neighbour\n",
    "sent='during the siege , george robertson had appointed shuja-ul-mulk , who was a bright boy only 12 years old and the youngest surviving son of aman-ul-mulk , as the ruler of chitral .'\n",
    "words_lst=[w for w in sent.split()]\n",
    "index=words_lst.index('bright')\n",
    "print (words_lst,index)\n",
    "nearest_neighbour(words_lst,vocab2elmo,index2vocab,30,index)\n",
    "# cosine_similarity([words_matrix[0][1]],words_matrix[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
