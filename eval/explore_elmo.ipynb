{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils.extmath import safe_sparse_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def context_independent_sim(x,y):\n",
    "# # use batch_to_ids to convert sentences to character ids\n",
    "#     sentences = [[x], [y]]\n",
    "#     character_ids = batch_to_ids(sentences)\n",
    "#     embeddings = elmo(character_ids)\n",
    "#     x1=embeddings['elmo_representations'][0][0].detach().numpy()\n",
    "#     x2=embeddings['elmo_representations'][0][1].detach().numpy()\n",
    "# #     print (x1)\n",
    "#     return cosine_similarity(x1,x2)\n",
    "    \n",
    "# allennlp.modules.scalar_mix.ScalarMix??\n",
    "\n",
    "\n",
    "# def context_independent(x):\n",
    "#     sentences=[[x]]\n",
    "#     character_ids = batch_to_ids(sentences)\n",
    "#     embeddings = elmo(character_ids)\n",
    "#     x1=embeddings['elmo_representations'][0][0].detach().numpy()\n",
    "#     return x1\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity2(X, Y=None, dense_output=True):\n",
    "    \"\"\"Compute cosine similarity between samples in X and Y.\n",
    "\n",
    "    Cosine similarity, or the cosine kernel, computes similarity as the\n",
    "    normalized dot product of X and Y:\n",
    "\n",
    "        K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "\n",
    "    On L2-normalized data, this function is equivalent to linear_kernel.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cosine_similarity>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray or sparse array, shape: (n_samples_X, n_features)\n",
    "        Input data.\n",
    "\n",
    "    Y : ndarray or sparse array, shape: (n_samples_Y, n_features)\n",
    "        Input data. If ``None``, the output will be the pairwise\n",
    "        similarities between all samples in ``X``.\n",
    "\n",
    "    dense_output : boolean (optional), default True\n",
    "        Whether to return dense output even when the input is sparse. If\n",
    "        ``False``, the output is sparse if both input arrays are sparse.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           parameter ``dense_output`` for dense output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kernel matrix : array\n",
    "        An array with shape (n_samples_X, n_samples_Y).\n",
    "    \"\"\"\n",
    "    # to avoid recursive import\n",
    "\n",
    "#     X, Y = check_pairwise_arrays(X, Y)\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    X_normalized = X / np.sqrt((X * X).sum())\n",
    "    if X is Y:\n",
    "        Y_normalized = X_normalized\n",
    "    else:\n",
    "        Y_normalized = Y / np.sqrt((Y * Y).sum())\n",
    "    \n",
    "    print (np.linalg.norm(X_normalized),np.linalg.norm(Y_normalized[0]))\n",
    "    K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)\n",
    "\n",
    "    return K\n",
    "\n",
    "def context_independent_batch(words,batchsize):\n",
    "    words_matrix=[]\n",
    "    for i in range(int(len(words)/batchsize)+1):\n",
    "        print (i)\n",
    "        start=i*batchsize\n",
    "        end=(i+1)*batchsize\n",
    "        if end > len(words):\n",
    "            end=len(words)\n",
    "        sentences=[[w] for w in words[start:end]]\n",
    "        character_ids = batch_to_ids(sentences)\n",
    "        embeddings = elmo(character_ids)\n",
    "        x_ls=embeddings['elmo_representations'][0].detach().numpy()\n",
    "        words_matrix.append(x_ls)\n",
    "        \n",
    "    words_matrix_all=np.concatenate(words_matrix,axis=0)\n",
    "#     words_matrix=words_matrix.reshape(words_matrix.shape[0],words_matrix.shape[2])\n",
    "    return words_matrix_all\n",
    "    \n",
    "    \n",
    "def context_dependent(x,index=0):\n",
    "    sentences=[x]\n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    embeddings = elmo(character_ids)\n",
    "    print (embeddings['elmo_representations'][0].shape)\n",
    "    x1=embeddings['elmo_representations'][0][0][index].detach().numpy()\n",
    "    return x1\n",
    "\n",
    "def nearest_neighbour(x,vocab,index2word,n_result,index=0):\n",
    "#     vocab = normalize(vocab)\n",
    "    s = np.sqrt((vocab * vocab).sum(1))\n",
    "    s[s==0.] = 1.\n",
    "    vocab /= s.reshape((s.shape[0], 1))\n",
    "    \n",
    "    x1=context_dependent(x,index)\n",
    "#     x1=normalize([x1])[0]\n",
    "    x1 = x1 / np.sqrt((x1 * x1).sum())\n",
    "\n",
    "#     x1=x1[0]\n",
    "#     print(np.linalg.norm(x1))\n",
    "#     similarity=cosine_similarity2([x1],vocab)[0]\n",
    "    similarity=vocab.dot(x1)\n",
    "    count=0\n",
    "    for i in (-similarity).argsort():\n",
    "                    if np.isnan(similarity[i]):\n",
    "                        continue\n",
    "                    print('{0}: {1}'.format(str(index2word[i]), str(similarity[i])))\n",
    "                    count += 1\n",
    "#                     top_words_i.append(i)\n",
    "#                     top_words.append(index2word[i])\n",
    "#                     similarity_scores.append(similarity[i])\n",
    "                    if count == n_result:\n",
    "                        break\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab\n",
    "try:\n",
    "    words_matrix=np.load('./lexsub_en/vocab2elmo.npy')\n",
    "    vocab2index=np.load('./lexsub_en/vocab2index.npy')\n",
    "    index2word={}\n",
    "    word2index={}\n",
    "    for vocab_index in vocab2index:\n",
    "        word2index[vocab_index[0]]=int(vocab_index[1])\n",
    "        index2word[int(vocab_index[1])]=vocab_index[0]\n",
    "except:\n",
    "    word2index={}\n",
    "    index2word={}\n",
    "    vocab_matrix=[]\n",
    "    words=[]\n",
    "    index=0\n",
    "    with open('../corpora/1-billion-vocab') as f:\n",
    "        for line in f:\n",
    "            word=line.split()[0]\n",
    "    #         vocab_matrix.append(context_independent(word))\n",
    "            index2word[int(index)]=word\n",
    "            word2index[word]=int(index)\n",
    "            words.append(word)\n",
    "            index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    words_matrix=context_independent_batch(words[:10000],1000)\n",
    "    len(words_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity simlex\n",
    "predicts=[]\n",
    "golds=[]\n",
    "line_num=0\n",
    "with open ('simlex') as f:\n",
    "    for line in f:\n",
    "        if line_num==0:\n",
    "            line_num+=1\n",
    "            continue\n",
    "        \n",
    "        line=line.split('\\t')\n",
    "        try:\n",
    "            predict = cosine_similarity(words_matrix[word2index[line[0]]],words_matrix[word2index[line[1]]])\n",
    "        except KeyError as e:\n",
    "            predict = cosine_similarity([context_dependent([line[0]])],[context_dependent([line[1]])])\n",
    "        \n",
    "        print (line[3],predict[0][0])\n",
    "\n",
    "        predicts.append(predict[0][0])\n",
    "\n",
    "        golds.append(line[3])\n",
    "\n",
    "        line_num+=1   \n",
    "          \n",
    "print ('simlex sim is {0}'.format(spearmanr(predicts,golds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEN\n",
    "predicts=[]\n",
    "golds=[]\n",
    "line_num=0\n",
    "with open ('MEN_dataset_lemma_form_full','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "    #         if line_num==0:\n",
    "#             line_num+=1\n",
    "#             continue\n",
    "        line=line.split()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            predict = cosine_similarity(words_matrix[word2index[line[0].split('-')[0]]],words_matrix[word2index[line[1].split('-')[0]]])\n",
    "        except KeyError as e:\n",
    "#             print (line[0].split('-')[0])\n",
    "            predict = cosine_similarity([context_dependent([line[0].split('-')[0]])],[context_dependent([line[1].split('-')[0]])])\n",
    "        \n",
    "        print (line[2],predict[0][0])\n",
    "\n",
    "        predicts.append(predict[0][0])\n",
    "\n",
    "        golds.append(float(line[2]))\n",
    "\n",
    "        line_num+=1   \n",
    "          \n",
    "print ('MEN sim is {0}'.format(spearmanr(predicts,golds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['during', 'the', 'siege', ',', 'george', 'robertson', 'had', 'appointed', 'shuja-ul-mulk', ',', 'who', 'was', 'a', 'bright', 'boy', 'only', '12', 'years', 'old', 'and', 'the', 'youngest', 'surviving', 'son', 'of', 'aman-ul-mulk', ',', 'as', 'the', 'ruler', 'of', 'chitral', '.'] 13\n",
      "torch.Size([1, 33, 1024])\n",
      "bright: 0.61784583\n",
      "brightest: 0.44194067\n",
      "brighter: 0.43457955\n",
      "brightly: 0.43435103\n",
      "willowy: 0.4324393\n",
      "brights: 0.42231432\n",
      "cute: 0.41112632\n",
      "lithe: 0.41022032\n",
      "blingy: 0.40745223\n",
      "dandyish: 0.40686908\n",
      "petite: 0.40597805\n",
      "brainy: 0.40514576\n",
      "dreamy: 0.40500307\n",
      "blonde: 0.4049771\n",
      "cheerful: 0.40490654\n",
      "crocky: 0.40411693\n",
      "bruery: 0.40315878\n",
      "ditsy: 0.40284222\n",
      "natty: 0.4012377\n",
      "raffish: 0.40094006\n",
      "jowly: 0.40067455\n",
      "waffly: 0.40000552\n",
      "brash: 0.3996535\n",
      "shiny: 0.3984455\n",
      "blondy: 0.39815348\n",
      "blond: 0.39763522\n",
      "dewy: 0.3975778\n",
      "lanky: 0.39620635\n",
      "croaky: 0.3949222\n",
      "bleek: 0.39472818\n"
     ]
    }
   ],
   "source": [
    "# nearest neighbour\n",
    "sent='during the siege , george robertson had appointed shuja-ul-mulk , who was a bright boy only 12 years old and the youngest surviving son of aman-ul-mulk , as the ruler of chitral .'\n",
    "words_lst=[w for w in sent.split()]\n",
    "index=words_lst.index('bright')\n",
    "print (words_lst,index)\n",
    "nearest_neighbour(words_lst,words_matrix,index2word,30,index)\n",
    "# cosine_similarity([words_matrix[0][1]],words_matrix[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
